{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning non-linear model dynamics from data using Dynamic Mode Decomposition  \n",
        "## ELEC ENG 475\n",
        "Ayush Gaggar  \n",
        "J.D. Peiffer  \n"
      ],
      "metadata": {
        "id": "kuF1GEzEjXQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction  \n",
        "Even with knowledge of a nonlinear system, modeling that system can be a quite difficult. Further, many nonlinear systems are functions of unknown inputs, or we may not be able to determine the inner workings of the model. However, this course has evidenced that it is not necessary to devolop a full model of the system to predict its behaivor, simply observing it gives us data to train different machine learning paradigms to approximate the unknown model's behavior.  \n",
        "\n",
        "This project uses Dynamic Mode Decomposition (DMD) to exploit low-dimensionality in recorded data without having to learn a set of equations governing some example non-linear systems. We then experiment with different inputs to the DMD algorithm and their effect on the \"learnability\" of the system.  \n",
        "\n",
        "## Problem Setup\n",
        "Here we assume we sample $N$ spatial points over $M$ regurarly spaced sample points.  \n",
        "The goal is to learn a matrix $A$ such that multiplying the current state of the measurements by $A$ gives a good approximation of the next state. This can be expressed as $$\\vec{x}_{j+1}=\\vec{A}\\vec{x}_j$$  \n",
        "This is also known as the Koopman operator, which is a _linear_ operator from one timepoint to the next.  \n",
        "We now concatanate all the $M$ snapshots into a single vector $$\\vec{X} = [\\vec{x}_1,\\vec{x}_2,...,\\vec{x}_{M-1}]$$  and also create another matrix $$\\vec{X}'= [\\vec{x}_2,\\vec{x}_3,...,\\vec{x}_{M}]$$\n",
        "To move from one step in time to another, we by definition multiply by $A$ giving $$\\vec{X}'=\\vec{A} \\vec{X}$$We can then solve for $$\\vec{A} = \\vec{X}'\\vec{X}^\\dagger$$ where $\\vec{X}^\\dagger$ is the psuedo-inverse of $\\vec{X}$. While $\\vec{A}$ may be different for different timesteps, the psuedo-inverse gives the least-squares fit across all timesteps. This method is remisicent of the analytical solution of linear regression with a least squares cost function.  \n",
        "\n",
        "Finally, it is worth noting that the real DMD algorithm does not use the full $\\vec{X}$. Instead, DMD uses Singular Value Decomposition to calculate $\\vec{A}$ in a lower dimensional space of the data [REF NEEDED]. This project makes use of the python library [PyDMD](https://github.com/mathLab/PyDMD#documentation) for this.  \n",
        "Required libraries are:  \n",
        "numpy  \n",
        "sympy  \n",
        "matplotlib  \n",
        "pydmd  "
      ],
      "metadata": {
        "id": "2ojGOuvYmNgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6dkWEHBZ2MQL"
      }
    }
  ]
}